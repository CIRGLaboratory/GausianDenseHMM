2,5c2,3
< from hmmlearn.base import ConvergenceMonitor, check_array
< from hmmlearn.utils import log_mask_zero
< import tensorflow.compat.v1 as tf
< tf.disable_v2_behavior()
---
> from hmmlearn.base import ConvergenceMonitor, log_mask_zero, check_array
> import tensorflow as tf
8,14d5
< # import hmmlearn._utils as fhmmc
< # from pathlib import  Path
< # from Cython.Build import cythonize
< # import pyximport
< # pyximport.install(setup_args={"ext_modules": cythonize(f"{Path(__file__).parent.resolve()}/hmmc/_hmmc.pyx", include_path=[np.get_include()]),
< #                               "include_dirs": [np.get_include()]})
< # from hmmc import _hmmc as _hmmcmod
18c9,10
< from utils import check_arr, pad_to_seqlen, check_random_state, dict_get, check_dir, compute_stationary, empirical_coocs, \
---
> from utils import check_arr, pad_to_seqlen, check_random_state, dict_get, check_dir, compute_stationary, \
>     empirical_coocs, \
20c12
< 
---
> from .models import HMMLoggingMonitor
22d13
< import itertools
24,64d14
< class HMMLoggingMonitor:
<     
<     def __init__(self, log_config=None):
<         
<         # Default log_config
<         self.log_config = {
<                 'exp_folder': None,  # root experiment folder
<                 #'plot_folder': None, # folder to store plots in
<                 'log_folder': None, # folder to store array-data in
<                 'metrics_initial' : True,  # whether to compute metrics before first estep
<                 'metrics_after_mstep_every_n_iter': 1, # frequency of computing metrics after mstep or none
<                 'metrics_after_estep_every_n_iter': 1, # frequency of computing metrics after estep or none
<                 'metrics_after_convergence': True, # whether to compute metrics after the training
<                 'gamma_after_estep': False, # whether to compute gammas (they are very large!)
<                 'gamma_after_mstep': False,
<                 'test_gamma_after_estep': False,  # whether to compute test gammas ..
<                 'test_gamma_after_mstep': False,
<                 'samples_after_estep': None,  # (n_seqs, seqlen) sample to draw after estep or None
<                 'samples_after_mstep': None,   # (n_seqs, seqlen) sample to draw after mstep or None
<                 'samples_after_cooc_opt': None # (n_seqs, seqlen) sample to draw after fitting model's coocs
<                 }
<         
<         if log_config is not None:
<             self.log_config.update(dict(log_config))
<     
<     def _check_log_path(self):
<         log_conf = self.log_config
<         exp_folder = '.' if log_conf['exp_folder'] is None else log_conf['exp_folder']
<         log_folder = '/data' if log_conf['log_folder'] is None else log_conf['log_folder']
<         log_path = check_dir(exp_folder + log_folder)
<         return log_path
<         
<     def log(self, file_name, log_dict, key_func=None):#stats, em_iter, ident):
<         log_path = self._check_log_path()
<         self._log(log_path, file_name, log_dict, key_func)
<         
<     def _log(self, log_path, file_name, log_dict, key_func=None):
<         if key_func is not None:
<             np.savez_compressed(log_path + '/' + file_name, **{key_func(key): log_dict[key] for key in log_dict.keys()})
<         else:
<             np.savez_compressed(log_path + '/' + file_name, **log_dict)
66,71c16
<     def emname(self, em_iter, ident):
<         return "logs_em=%d_ident=%s" % (int(em_iter), ident)
<     
< class GammaMultinomialHMM(MultinomialHMM):
<             
<         
---
> class GammaGaussianHMM(GaussianHMM):
73,74c18,22
<     def __init__(self, n_hidden_states=1, n_observables=None,
<                  startprob_prior=1.0, transmat_prior=1.0, 
---
> 
>     def __init__(self, n_hidden_states=1, n_dims=None,
>                  covariance_type='diag', min_covar=0.001,
>                  startprob_prior=1.0, transmat_prior=1.0,
>                  means_prior=0, means_weight=0, covars_prior=0.01, covars_weight=1,
76,77c24,25
<                 params="ste", init_params="ste", logging_monitor=None):
<     
---
>                  params="ste", init_params="ste", logging_monitor=None):
> 
80c28
<             init_params = "ste"
---
>             init_params = "stmc"
84,96d31
<         self.n_observables = n_observables
<         self.em_iter = em_iter
<         self.n_hidden_states = n_hidden_states
<         super(GammaMultinomialHMM, self).__init__(n_components=n_hidden_states,
<                                 startprob_prior=startprob_prior,
<                                 transmat_prior=transmat_prior,
<                                 algorithm="viterbi",
<                                 random_state=random_state,
<                                 n_iter=em_iter,
<                                 tol=convergence_tol,
<                                 verbose=verbose,
<                                 params=params,
<                                 init_params=init_params)
97a33,49
>         super(GammaGaussianHMM, self).__init__(n_components=n_hidden_states,
>                                                covariance_type=covariance_type,
>                                                min_covar=min_covar,
>                                                startprob_prior=startprob_prior,
>                                                transmat_prior=transmat_prior,
>                                                means_prior=means_prior,
>                                                means_weight=means_weight,
>                                                covars_prior=covars_prior,
>                                                covars_weight=covars_weight,
>                                                algorithm="viterbi",
>                                                random_state=random_state,
>                                                n_iter=em_iter,
>                                                tol=convergence_tol,
>                                                verbose=verbose,
>                                                params=params,
>                                                init_params=init_params,
>                                                implementation='log')
98a51
>         self.n_dims = n_dims
100,101c53,54
<         self.convergence_tol = convergence_tol
<         if self.matrix_initializer is not None and self.n_observables is not None:
---
> 
>         if self.matrix_initializer is not None and self.n_dims is not None:
103c56
<         
---
> 
107,108c60,61
<         gamma_pairwise = np.zeros((n_seqs, max_seqlen-1, self.n_components, self.n_components))
<         bar_gamma_pairwise = np.zeros((max_seqlen-1, self.n_components, self.n_components))
---
>         gamma_pairwise = np.zeros((n_seqs, max_seqlen - 1, self.n_components, self.n_components))
>         bar_gamma_pairwise = np.zeros((max_seqlen - 1, self.n_components, self.n_components))
110c63
<         
---
> 
113c66
<             
---
> 
115c68
<             - start: array, shape (n_hidden_states,); 
---
>             - start: array, shape (n_hidden_states,);
117c70
<                      probability of the first sample being generated by the i-th 
---
>                      probability of the first sample being generated by the i-th
125c78
<             
---
> 
136c89
<             
---
> 
139,141c92,94
<         
<         stats = super(GammaMultinomialHMM, self)._initialize_sufficient_statistics()
<         
---
> 
>         stats = super(GammaGaussianHMM, self)._initialize_sufficient_statistics()
> 
144c97
<         
---
> 
147c100
<         
---
> 
149c102
<         
---
> 
151c104
<     
---
> 
155c108,109
<     """ 
---
>     """
> 
157,160c111,114
<         
<         X, n_seqs, max_seqlen = check_arr(X, lengths)
<         
<         # This initializes the transition matrices: 
---
> 
>         X, n_seqs, max_seqlen = check_arr_gaussian(X, lengths)
> 
>         # This initializes the transition matrices:
166c120
<         super(GammaMultinomialHMM, self)._init(X.transpose())
---
>         super(GammaGaussianHMM, self)._init(X)
168,170c122,124
<         if self.n_observables is None:
<             self.n_observables = self.n_features
<         elif self.n_features != self.n_observables:
---
>         if self.n_dims is None:
>             self.n_dims = self.n_features
>         elif self.n_features != self.n_dims:
172,173c126,127
<                             "%d unique symbols" % (self.n_observables, self.n_features))
<         
---
>                             "%d unique symbols" % (self.n_dims, self.n_features))
> 
176c130
<             
---
> 
178c132
<             
---
> 
181,182c135,136
<         # If random_state is int, a new np.RandomState instance seeded with random_state 
<         # is returned. If random_state is already an instance of np.RandomState, 
---
>         # If random_state is int, a new np.RandomState instance seeded with random_state
>         # is returned. If random_state is already an instance of np.RandomState,
186c140
<         pi, A, B = matrix_initializer(self.n_components, self.n_observables, self.random_state)
---
>         pi, A, B = matrix_initializer(self.n_components, self.n_dims, self.random_state)
188c142
<         self.emissionprob_ = B
---
>         self.emissionprob_ = B  # TODO
190c144
<                 
---
> 
209,210c163,164
<         
<         # Initializes the n_features (number of observables),
---
> 
>         # Initializes the n_dims (number of dimensions),
214c168
<         
---
> 
218c172
<         
---
> 
222,224c176,178
<         stats = self._initialize_sufficient_statistics(n_seqs, max_seqlen)
<         for iter in range(self.n_iter):
<             # print(stats)
---
>         for iter in tqdm(range(self.n_iter), desc="Fit model"):
> 
>             stats = self._initialize_sufficient_statistics(n_seqs, max_seqlen)
226d179
<             # stats = self._initialize_sufficient_statistics(n_seqs, max_seqlen)
231c184
<             
---
> 
233d185
<             # print(stats['trans'])
235c187
<             # print("E step done")
---
> 
241,243c193
<             # print(self.transmat_)
<             # print(stats['trans'])
<             # print("M step start")
---
> 
247c197
<             # print("M step done")
---
> 
254c204
<             #if self.monitor_.converged:
---
>             # if self.monitor_.converged:
257c207
<             
---
> 
265,274c215,216
<     # def forward_log(self, framelogprob):
<     #     n_observations = framelogprob.shape[0]
<     #     state_combinations = [tuple(x) for x in list(itertools.product(np.arange(self.n_components), repeat=self.n_chains))]
<     #     fwdlattice = np.zeros((n_observations, self.n_components ** self.n_chains))
<     #     fhmmc._forward(n_observations, self.n_chains, self.n_components, state_combinations, self.log_startprob,
<     #                    self.log_transmat, framelogprob, fwdlattice)
<     #     return tf.reduce_logsumexp(fwdlattice[-1]), fwdlattice
<     
<     def _forward_backward_gamma_pass(self, X, lengths=None, stats=None, params=None):
<         
---
>     def _forward_backward_gamma_pass(self, X, lengths=None, stats=None, params=None):  # TODO: change emmision
> 
276c218
<         
---
> 
278c220
<             X, n_seqs, max_seqlen = check_arr(X)
---
>             X, n_seqs, max_seqlen = check_arr_gaussian(X, lengths)
280c222
<         
---
> 
282c224
<         
---
> 
285c227
<             
---
> 
289,295c231,235
<             framelogprob = self._compute_log_likelihood(X[i:j].transpose())
<             logprob, fwdlattice = self._do_forward_log_pass(framelogprob)
<             fwdlattice = _hmmcmod.forward_log(self.startprob_, self.transmat_, framelogprob)
<             # print(logprob)
<             # print(fwdlattice)
<             stats['all_logprobs'][seq_idx] = logprob # logprob = probability of X[i:j]
<             
---
>             framelogprob = self._compute_log_likelihood(X[i:j])
>             logprob, fwdlattice = self._do_forward_pass(framelogprob)
> 
>             stats['all_logprobs'][seq_idx] = logprob  # logprob = probability of X[i:j]
> 
297,299c237,239
<             bwdlattice = _hmmcmod.backward_log(self.startprob_, self.transmat_, framelogprob)
<             posteriors = self._compute_posteriors_log(fwdlattice, bwdlattice)
<             
---
>             bwdlattice = self._do_backward_pass(framelogprob)
>             posteriors = self._compute_posteriors(fwdlattice, bwdlattice)
> 
312,319c252,256
<             log_xi_sum = _hmmcmod.compute_log_xi_sum(fwdlattice,
<                                       self.transmat_,
<                                       bwdlattice, framelogprob)
<             # _hmmcmod.compute_log_xi_sum(fwdlattice,
<             #                             self.transmat_,
<             #                             bwdlattice, framelogprob)
<             
<             
---
>             _hmmcmod._compute_log_xi_sum(n_samples, n_components, fwdlattice,
>                                          log_mask_zero(self.transmat_),
>                                          bwdlattice, framelogprob,
>                                          log_xi_sum, cur_gamma_pairwise)
> 
334c271
<         
---
> 
337,340c274,276
<     
<     # Currently supported: Total log-likelihood on given data, 
<     # individual log-likelihoods 
<     def _compute_metrics(self, X, lengths, stats, em_iter, ident, 
---
>     # Currently supported: Total log-likelihood on given data,
>     # individual log-likelihoods
>     def _compute_metrics(self, X, lengths, stats, em_iter, ident,
342,343c278
<         
<         
---
> 
347c282
<         
---
> 
359c294
<         
---
> 
361,362c296,298
<         log_dict['startprob'], log_dict['transmat'], log_dict['emissionprob'] = self.startprob_, self.transmat_, self.emissionprob_
<         
---
>         log_dict['startprob'], log_dict['transmat'], log_dict[
>             'emissionprob'] = self.startprob_, self.transmat_, self.emissionprob_
> 
365c301
<             
---
> 
368,369c304,306
<                 log_dict['gamma_pairwise'], log_dict['bar_gamma_pairwise'] = stats['gamma_pairwise'], stats['bar_gamma_pairwise']
<             
---
>                 log_dict['gamma_pairwise'], log_dict['bar_gamma_pairwise'] = stats['gamma_pairwise'], stats[
>                     'bar_gamma_pairwise']
> 
373c310
<             
---
> 
375,390c312,326
<             #log_dict['all_loglikes'] = self.score_individual_sequences(X, lengths)[0]
<             #log_dict['loglike'] = np.sum(log_dict['all_loglikes']) #self.score(X, lengths)
<             
<             #print("In _compute_metrics aE", log_dict['all_loglikes'].shape, stats['all_logprobs'].shape)
<             #print('stats sum:', np.sum(stats['all_logprobs']), log_dict['loglike'])
<             #_al = self.score_individual_sequences(X, lengths)[0]
<             #print(np.sum(stats['all_logprobs']) == np.sum(_al))
<             #print(np.all(stats['all_logprobs'] == _al))
<             #print('stats all logprobs', stats['all_logprobs'])
<             #print('all loglikes', log_dict['all_loglikes'])
<             #_scored_loglikes = np.array(log_dict['all_loglikes'])
<             #_stat_loglikes = np.array(stats['all_logprobs'])
<             #print(np.all(_scored_loglikes == _stat_loglikes))
<             #print(np.sum(_scored_loglikes) == np.sum(_stat_loglikes))
<             
<             
---
>             # log_dict['all_loglikes'] = self.score_individual_sequences(X, lengths)[0]
>             # log_dict['loglike'] = np.sum(log_dict['all_loglikes']) #self.score(X, lengths)
> 
>             # print("In _compute_metrics aE", log_dict['all_loglikes'].shape, stats['all_logprobs'].shape)
>             # print('stats sum:', np.sum(stats['all_logprobs']), log_dict['loglike'])
>             # _al = self.score_individual_sequences(X, lengths)[0]
>             # print(np.sum(stats['all_logprobs']) == np.sum(_al))
>             # print(np.all(stats['all_logprobs'] == _al))
>             # print('stats all logprobs', stats['all_logprobs'])
>             # print('all loglikes', log_dict['all_loglikes'])
>             # _scored_loglikes = np.array(log_dict['all_loglikes'])
>             # _stat_loglikes = np.array(stats['all_logprobs'])
>             # print(np.all(_scored_loglikes == _stat_loglikes))
>             # print(np.sum(_scored_loglikes) == np.sum(_stat_loglikes))
> 
401c337
<         
---
> 
403c339
<             # print(log_dict)
---
> 
407,408c343,344
<             log_dict['loglike'] = np.sum(log_dict['all_loglikes']) #self.score(X, lengths)
<             
---
>             log_dict['loglike'] = np.sum(log_dict['all_loglikes'])  # self.score(X, lengths)
> 
411,412c347,349
<                 log_dict['gamma_pairwise'], log_dict['bar_gamma_pairwise'] = stats['gamma_pairwise'], stats['bar_gamma_pairwise']
<             
---
>                 log_dict['gamma_pairwise'], log_dict['bar_gamma_pairwise'] = stats['gamma_pairwise'], stats[
>                     'bar_gamma_pairwise']
> 
423c360
<         
---
> 
426,428c363,366
<             
<             if log_config['test_gamma_after_estep'] and ident == 'aE' or log_config['test_gamma_after_mstep'] and ident == 'aM':
<                 
---
> 
>             if log_config['test_gamma_after_estep'] and ident == 'aE' or log_config[
>                 'test_gamma_after_mstep'] and ident == 'aM':
> 
437c375
<             
---
> 
439,440c377,379
<                 log_dict['val_all_loglikes'], log_dict['val_loglike'] = self.score_individual_sequences(val, val_lengths)
<         
---
>                 log_dict['val_all_loglikes'], log_dict['val_loglike'] = self.score_individual_sequences(val,
>                                                                                                         val_lengths)
> 
442,443c381,383
<     
<     """ Sample n_seqs (int) sequences each of length seqlen (int). Returns an array of shape (n_seqs, seqlen, n_features) """
---
> 
>     """ Sample n_seqs (int) sequences each of length seqlen (int). Returns an array of shape (n_seqs, seqlen, n_dims) """
> 
447c387
<     
---
> 
452c392
<         X : array-like, shape (n_samples, n_features)
---
>         X : array-like, shape (n_samples, n_dims)
467c407
<         #_utils.check_is_fitted(self, "startprob_")
---
>         # _utils.check_is_fitted(self, "startprob_")
470c410
<         
---
> 
476c416
<             
---
> 
481,482c421,422
<             framelogprob = self._compute_log_likelihood(X[i:j].transpose())
<             logprobij, _fwdlattice = self._do_forward_log_pass(framelogprob) # TODO
---
>             framelogprob = self._compute_log_likelihood(X[i:j])
>             logprobij, _fwdlattice = self._do_forward_pass(framelogprob)
486c426
<     
---
> 
489a430
> 
491c432
<         
---
> 
494,495c435,436
<         
<         # X has shape (seqs, 1); 
---
> 
>         # X has shape (seqs, 1);
500,501c441,442
<             arr[idx] = np.pad(sequence, 
<                               (0, max_seqlen - len(sequence)), 
---
>             arr[idx] = np.pad(sequence,
>                               (0, max_seqlen - len(sequence)),
503,504c444,445
<         
<         # Check if arr contains only integer 
---
> 
>         # Check if arr contains only integer
510d450
<                                 
512,531c452,546
< class StandardHMM(GammaMultinomialHMM):
<         
<     def __init__(self, n_hidden_states=1, n_observables=None,
<              startprob_prior=1.0, transmat_prior=1.0, 
<              random_state=None, em_iter=10, convergence_tol=1e-2, verbose=False,
<             params="ste", init_params="ste", logging_monitor=None):
<     
<         super(StandardHMM, self).__init__(n_hidden_states=n_hidden_states,
<                                      n_observables=n_observables,
<                                     startprob_prior=startprob_prior,
<                                     transmat_prior=transmat_prior,
<                                     random_state=random_state,
<                                     em_iter=em_iter,
<                                     convergence_tol=convergence_tol,
<                                     verbose=verbose,
<                                     params=params,
<                                     init_params=init_params,
<                                     logging_monitor=logging_monitor)
<         
<     def _compute_metrics(self, X, lengths, stats, em_iter, ident, 
---
>     class StandardHMM(GammaMultinomialHMM):
> 
>         def __init__(self, n_hidden_states=1, n_observables=None,
>                      startprob_prior=1.0, transmat_prior=1.0,
>                      random_state=None, em_iter=10, convergence_tol=1e-2, verbose=False,
>                      params="ste", init_params="ste", logging_monitor=None):
> 
>             super(StandarGaussiandHMM, self).__init__(n_hidden_states=n_hidden_states,
>                                                       n_observables=n_observables,
>                                                       startprob_prior=startprob_prior,
>                                                       transmat_prior=transmat_prior,
>                                                       random_state=random_state,
>                                                       em_iter=em_iter,
>                                                       convergence_tol=convergence_tol,
>                                                       verbose=verbose,
>                                                       params=params,
>                                                       init_params=init_params,
>                                                       logging_monitor=logging_monitor)
> 
>         def _compute_metrics(self, X, lengths, stats, em_iter, ident,
>                              val=None, val_lengths=None):
> 
>             log_config = self.logging_monitor.log_config
>             log_dict = super(StandarGaussiandHMM, self)._compute_metrics(X, lengths, stats, em_iter, ident, val,
>                                                                          val_lengths)
> 
>             gamma, bar_gamma = stats['gamma'], stats['bar_gamma']
>             bar_gamma_pairwise = stats['bar_gamma_pairwise']
>             log_dict['train_losses'] = self._compute_loss(X, lengths, bar_gamma, bar_gamma_pairwise, gamma)
> 
>             if val is not None:
> 
>                 log_dict['test_losses'] = self._compute_loss(val, val_lengths, bar_gamma, bar_gamma_pairwise, gamma)
> 
>                 # val-gammas are not necessarily in log_dict
>                 val_bar_gamma = dict_get(log_dict, 'val_bar_gamma')
>                 val_bar_gamma_pairwise = dict_get(log_dict, 'val_bar_gamma_pairwise')
>                 val_gamma = dict_get(log_dict, 'val_gamma')
> 
>                 if val_bar_gamma is not None and val_bar_gamma_pairwise is not None and val_gamma is not None:
>                     log_dict['test_gamma_losses'] = self._compute_loss(val, val_lengths, val_bar_gamma,
>                                                                        val_bar_gamma_pairwise, val_gamma)
> 
>             return log_dict
> 
>         """ Computes loss on given sequence; Using given gamma terms and
>         the current transition matrices
>         """
> 
>         def _compute_loss(self, X, lengths, bar_gamma, bar_gamma_pairwise, gamma):
> 
>             X, n_seqs, max_seqlen = self._observations_to_padded_matrix(X, lengths)
> 
>             log_A = np.log(self.transmat_)
>             log_B = np.log(self.emissionprob_)
>             log_pi = np.log(self.startprob_)
> 
>             tilde_B = log_B[:, X]  # Has shape (n_hidden_states, seqs, max_seqlen)
> 
>             loss1 = -np.einsum('s,s->', log_pi, bar_gamma[0, :])
>             loss2 = -np.einsum('jl,tjl->', log_A, bar_gamma_pairwise)
>             loss3 = -np.einsum('jit,itj->', tilde_B, gamma)
>             loss = loss1 + loss2 + loss3
> 
>             return np.array([loss, loss1, loss2, loss3])
> 
> 
> class StandardGaussianHMM(GammaGaussianHMM):
> 
>     def __init__(self, n_hidden_states=1, n_dims=None,
>                  covariance_type='diag', min_covar=0.001,
>                  startprob_prior=1.0, transmat_prior=1.0,
>                  means_prior=0, means_weight=0, covars_prior=0.01, covars_weight=1,
>                  random_state=None, em_iter=10, convergence_tol=1e-2, verbose=False,
>                  params="ste", init_params="ste", logging_monitor=None):
> 
>         super(StandardGaussianHMM, self).__init__(n_hidden_states=n_hidden_states,
>                                                   n_dims=n_dims,
>                                                   covariance_type=covariance_type,
>                                                   min_covar=min_covar,
>                                                   startprob_prior=startprob_prior,
>                                                   transmat_prior=transmat_prior,
>                                                   means_prior=means_prior,
>                                                   means_weight=means_weight,
>                                                   covars_prior=covars_prior,
>                                                   covars_weight=covars_weight,
>                                                   random_state=random_state,
>                                                   em_iter=em_iter,
>                                                   convergence_tol=convergence_tol,
>                                                   verbose=verbose,
>                                                   params=params,
>                                                   init_params=init_params,
>                                                   logging_monitor=logging_monitor)
> 
>     def _compute_metrics(self, X, lengths, stats, em_iter, ident,
533c548
<         
---
> 
535,536c550,552
<         log_dict = super(StandardHMM, self)._compute_metrics(X, lengths, stats, em_iter, ident, val, val_lengths)
<         
---
>         log_dict = super(StandarGaussiandHMM, self)._compute_metrics(X, lengths, stats, em_iter, ident, val,
>                                                                      val_lengths)
> 
540c556
<         
---
> 
542c558
<             
---
> 
544c560
<             
---
> 
549c565
<             
---
> 
551,552c567,569
<                 log_dict['test_gamma_losses'] = self._compute_loss(val, val_lengths, val_bar_gamma, val_bar_gamma_pairwise, val_gamma)
<     
---
>                 log_dict['test_gamma_losses'] = self._compute_loss(val, val_lengths, val_bar_gamma,
>                                                                    val_bar_gamma_pairwise, val_gamma)
> 
554c571
<     
---
> 
557a575
> 
559c577
<         
---
> 
561c579
<         
---
> 
565,567c583,585
<         
<         tilde_B = log_B[:, X] # Has shape (n_hidden_states, seqs, max_seqlen)
<         
---
> 
>         tilde_B = log_B[:, X]  # Has shape (n_hidden_states, seqs, max_seqlen)
> 
570c588
<         loss3 = -np.einsum('jit,itj->', tilde_B, gamma) 
---
>         loss3 = -np.einsum('jit,itj->', tilde_B, gamma)
572c590
<         
---
> 
574d591
<     
576,578c593,595
< class DenseHMM(GammaMultinomialHMM):
<     
<     SUPPORTED_REPRESENTATIONS = frozenset(('uvwzz0', 'vzz0'))
---
> 
> class GaussianDenseHMM(GammaGaussianHMM):
>     SUPPORTED_REPRESENTATIONS = frozenset(('uzz0-musigma', 'vzz0'))
580,599c597,623
<     
<     def __init__(self, n_hidden_states=1, n_observables=None,
<              startprob_prior=1.0, transmat_prior=1.0, 
<              random_state=None, em_iter=10, convergence_tol=1e-2, verbose=False,
<             params="ste", init_params="ste", logging_monitor=None, mstep_config=None,
<                 opt_schemes=None):
<     
<         super(DenseHMM, self).__init__(n_hidden_states=n_hidden_states,
<                                      n_observables=n_observables,
<                                     startprob_prior=startprob_prior,
<                                     transmat_prior=transmat_prior,
<                                     random_state=random_state,
<                                     em_iter=em_iter,
<                                     convergence_tol=convergence_tol,
<                                     verbose=verbose,
<                                     params=params,
<                                     init_params=init_params,
<                                     logging_monitor=logging_monitor)
<         
<         # self.convergence_tol = convergence_tol
---
> 
>     def __init__(self, n_hidden_states=1, n_dims=None,
>                  covariance_type='diag', min_covar=0.001,
>                  startprob_prior=1.0, transmat_prior=1.0,
>                  means_prior=0, means_weight=0, covars_prior=0.01, covars_weight=1,
>                  random_state=None, em_iter=10, convergence_tol=1e-2, verbose=False,
>                  params="ste", init_params="ste", logging_monitor=None,
>                  mstep_config=None, opt_schemes=None):
> 
>         super(GaussianDenseHMM, self).__init__(n_hidden_states=n_hidden_states,
>                                                n_dims=n_dims,
>                                                covariance_type=covariance_type,
>                                                min_covar=min_covar,
>                                                startprob_prior=startprob_prior,
>                                                transmat_prior=transmat_prior,
>                                                means_prior=means_prior,
>                                                means_weight=means_weight,
>                                                covars_prior=covars_prior,
>                                                covars_weight=covars_weight,
>                                                random_state=random_state,
>                                                em_iter=em_iter,
>                                                convergence_tol=convergence_tol,
>                                                verbose=verbose,
>                                                params=params,
>                                                init_params=init_params,
>                                                logging_monitor=logging_monitor)
> 
601c625
<         
---
> 
603d626
<         self.mstep_config = mstep_config
605c628
<                      
---
> 
607c630
<         self.initializer = dict_get(mstep_config, 'initializer', default=tf.initializers.random_normal(0.,1.))
---
>         self.initializer = dict_get(mstep_config, 'initializer', default=tf.initializers.random_normal(0., 1.))
609,611c632,634
<         self.l_vw = dict_get(mstep_config, 'l_musigma', default=3)
<         self.trainables = dict_get(mstep_config, 'trainables', default='uvwzz0')
<         self.representations = dict_get(mstep_config, 'representations', default='uvwzz0')
---
>         self.l_musigma = dict_get(mstep_config, 'l_musigma', default=3)
>         self.trainables = dict_get(mstep_config, 'trainables', default='uzz0-musigma')
>         self.representations = dict_get(mstep_config, 'representations', default='uzz0-musigma')
613c636
<         
---
> 
615c638
<         self.init_ = None # Variable initializer
---
>         self.init_ = None  # Variable initializer
618,620c641,643
<         self.u, self.v, self.w, self.z, self.z0 = None, None, None, None, None # Representations
<         self.A_from_reps_hmmlearn, self.B_from_reps_hmmlearn, self.pi_from_reps_hmmlearn = None, None, None # HMM parameters
<         
---
>         self.u, self.mu, self.sigma, self.z, self.z0 = None, None, None, None, None  # Representations
>         self.A_from_reps_hmmlearn, self.B_from_reps_hmmlearn, self.pi_from_reps_hmmlearn = None, None, None  # HMM parameters
> 
623,624c646,648
<         self.em_lr = dict_get(mstep_config, 'em_lr', default=0.1) # TODO: Actually only need em_optimizer
<         self.em_optimizer = dict_get(mstep_config, 'em_optimizer', default=tf.compat.v1.train.GradientDescentOptimizer(self.em_lr))
---
>         self.em_lr = dict_get(mstep_config, 'em_lr', default=0.1)  # TODO: Actually only need em_optimizer
>         self.em_optimizer = dict_get(mstep_config, 'em_optimizer',
>                                      default=tf.compat.v1.train.GradientDescentOptimizer(self.em_lr))
626,627c650,651
<         self.gamma, self.bar_gamma, self.bar_gamma_pairwise = None, None, None # Placeholders
<         self.tilde_O, self.tilde_O_ph = None, None # Input sequence
---
>         self.gamma, self.bar_gamma, self.bar_gamma_pairwise = None, None, None  # Placeholders
>         self.tilde_O, self.tilde_O_ph = None, None  # Input sequence
629,630c653,654
<         self.loss_scaled, self.loss_update = None, None # Loss to optimize
<         
---
>         self.loss_scaled, self.loss_update = None, None  # Loss to optimize
> 
632,633c656,658
<         self.cooc_lr = dict_get(mstep_config, 'cooc_lr', default=0.01) # TODO actually only need optimizer
<         self.cooc_optimizer = dict_get(mstep_config, 'cooc_optimizer', default=tf.compat.v1.train.GradientDescentOptimizer(self.cooc_lr))
---
>         self.cooc_lr = dict_get(mstep_config, 'cooc_lr', default=0.01)  # TODO actually only need optimizer
>         self.cooc_optimizer = dict_get(mstep_config, 'cooc_optimizer',
>                                        default=tf.compat.v1.train.GradientDescentOptimizer(self.cooc_lr))
635,636c660,661
<         self.loss_cooc, self.loss_cooc_update = None, None 
<         self.A_stationary = None 
---
>         self.loss_cooc, self.loss_cooc_update = None, None
>         self.A_stationary = None
638,641c663,666
<         
<     
<     def _build_tf_em_graph(self, A_log_ker, B_log_ker, pi_log_ker, A_log_ker_normal, B_log_ker_normal, pi_log_ker_normal):
<         
---
> 
>     def _build_tf_em_graph(self, A_log_ker, B_log_ker, pi_log_ker, A_log_ker_normal, B_log_ker_normal,
>                            pi_log_ker_normal):
> 
643d667
<             
645c669
<             gamma = tf.placeholder(name="gamma", dtype=tf.float64, 
---
>             gamma = tf.placeholder(name="gamma", dtype=tf.float64,
649c673
<             bar_gamma_pairwise = tf.placeholder(name="bar_gamma_pairwise", 
---
>             bar_gamma_pairwise = tf.placeholder(name="bar_gamma_pairwise",
651c675,676
<                                                 shape=[None, self.n_components, self.n_components]) #TODO gamma or bar_gamma?
---
>                                                 shape=[None, self.n_components,
>                                                        self.n_components])  # TODO gamma or bar_gamma?
653,656c678,679
<                                shape=[None, None, self.n_observables])
<             
<             
<             
---
>                                         shape=[None, None, self.n_observables])
> 
663c686
<             
---
> 
668c691
<             
---
> 
672c695
<             
---
> 
676,679c699,702
<             
<             loss_total = tf.identity(loss_1 + loss_1_normalization + 
<                                      loss_2 + loss_2_normalization + 
<                                      loss_3 + loss_3_normalization, 
---
> 
>             loss_total = tf.identity(loss_1 + loss_1_normalization +
>                                      loss_2 + loss_2_normalization +
>                                      loss_3 + loss_3_normalization,
691c714
<             
---
> 
694c717
<             
---
> 
696c719
<     
---
> 
698,699d720
<         
<         with self.graph.as_default():
700a722
>         with self.graph.as_default():
703c725,726
<             A_stationary = tf.placeholder(dtype=tf.float64, shape=[self.n_components]) # Assumed to be the eigenvector of A.T
---
>             A_stationary = tf.placeholder(dtype=tf.float64,
>                                           shape=[self.n_components])  # Assumed to be the eigenvector of A.T
718,719c741
<     
<         
---
> 
721,722c743
<         
<         
---
> 
724c745
<             raise Exception("Given representation argument is invalid. Has to be one of %s" % 
---
>             raise Exception("Given representation argument is invalid. Has to be one of %s" %
727c748
<         if self.representations == 'vzz0' and self.l_vw != self.l_uz:
---
>         if self.representations == 'vzz0' and self.l_musigma != self.l_uz:
730d750
< 
732,733c752,754
<             raise Exception("Given unsupported optimization scheme! Supported are: %s" % str(self.SUPPORTED_OPT_SCHEMES))
<         
---
>             raise Exception(
>                 "Given unsupported optimization scheme! Supported are: %s" % str(self.SUPPORTED_OPT_SCHEMES))
> 
735c756
<         
---
> 
737c758
<             
---
> 
740c761
<             v = tf.compat.v1.get_variable(name="v", dtype=tf.float64, shape=[self.n_observables, self.l_vw], 
---
>             u = tf.get_variable(name="u", dtype=tf.float64, shape=[self.n_components, self.l_uz],
742,754d762
<                                 trainable=('v' in self.trainables))
<             z = tf.compat.v1.get_variable(name="z", dtype=tf.float64, shape=[self.l_uz, self.n_components], 
<                                 initializer=self.initializer, 
<                                 trainable=('z' in self.trainables and 
<                                            ('z0' not in self.trainables 
<                                             or 'zz0' in self.trainables)))
<             z0 = tf.compat.v1.get_variable(name="z0", dtype=tf.float64, shape=[self.l_uz, 1], 
<                                 initializer=self.initializer,
<                                  trainable=('z0' in self.trainables))
<             
<             if self.representations == 'uvwzz0':
<                 u = tf.compat.v1.get_variable(name="u", dtype=tf.float64, shape=[self.n_components, self.l_uz], 
<                                 initializer=self.initializer, 
756,757c764,765
<             
<                 w = tf.compat.v1.get_variable(name="w", dtype=tf.float64, shape=[self.l_vw, self.n_components], 
---
> 
>             z = tf.get_variable(name="z", dtype=tf.float64, shape=[self.l_uz, self.n_components],
759,760c767,782
<                                 trainable=('w' in self.trainables))
<             
---
>                                 trainable=('z' in self.trainables and
>                                            ('z0' not in self.trainables
>                                             or 'zz0' in self.trainables)))
>             z0 = tf.get_variable(name="z0", dtype=tf.float64, shape=[self.l_uz, 1],
>                                  initializer=self.initializer,
>                                  trainable=('z0' in self.trainables))
> 
>             if self.representations == 'uzz0-musigma':
>                 mu = tf.get_variable(name="mu", dtype=tf.float64, shape=[self.n_dims, self.l_musigma],
>                                     initializer=self.initializer,
>                                     trainable=('v' in self.trainables))
> 
>                 sigma = tf.get_variable(name="sigma", dtype=tf.float64, shape=[self.l_musigma, self.n_components],
>                                     initializer=self.initializer,
>                                     trainable=('w' in self.trainables))
> 
762c784
<                    
---
> 
764c786
<             if self.representations == 'uvwzz0': # Convention here: B is m x n
---
>             if self.representations == 'uvwzz0':  # Convention here: B is m x n
766c788
<                 B_scalars = tf.matmul(v, w, name="B_scalars")
---
>                 B_scalars = [mu, sigma]  # TODO
768c790
<                 
---
> 
770,773c792,795
<                 A_scalars = tf.matmul(z, z, transpose_a = True, name="A_scalars")
<                 B_scalars = tf.matmul(v, z, name="B_scalars")
<                 pi_scalars = tf.matmul(z0, z, transpose_a = True, name="pi_scalars")
<             
---
>                 A_scalars = tf.matmul(z, z, transpose_a=True, name="A_scalars")
>                 B_scalars = [mu, sigma]  # TODO
>                 pi_scalars = tf.matmul(z0, z, transpose_a=True, name="pi_scalars")
> 
776c798
<                 
---
> 
778c800
<                 B_from_reps = tf.nn.softmax(B_scalars, axis=0)
---
>                 B_from_reps = B_scalars
780,784c802,806
<                 
<                 A_log_ker_normal = tf.reduce_logsumexp(A_scalars, axis=0) # L
<                 B_log_ker_normal = tf.reduce_logsumexp(B_scalars, axis=0) #Convention: Columns = Distribution for one state
<                 pi_log_ker_normal = tf.reduce_logsumexp(pi_scalars) #L0
<                 
---
> 
>                 A_log_ker_normal = tf.reduce_logsumexp(A_scalars, axis=0)  # L
>                 B_log_ker_normal = B_scalars
>                 pi_log_ker_normal = tf.reduce_logsumexp(pi_scalars)  # L0
> 
788c810
<                 
---
> 
791c813
<                 B_scalars_ker = self.kernel(B_scalars)
---
>                 B_scalars_ker = B_scalars
793c815
<                 
---
> 
797c819
<                 
---
> 
801c823
<                 
---
> 
805c827
<                                       
---
> 
809,810c831,832
<             pi_from_reps_hmmlearn = tf.reshape(pi_from_reps, (-1,), name='pi_from_reps')        
<             
---
>             pi_from_reps_hmmlearn = tf.reshape(pi_from_reps, (-1,), name='pi_from_reps')
> 
814c836
<             
---
> 
817,818c839,841
<                 self.gamma, self.bar_gamma, self.bar_gamma_pairwise, self.tilde_O_ph, self.loss_update, self.loss_scaled, self.loss_1, self.loss_1_normalization, self.loss_2, self.loss_2_normalization, self.loss_3, self.loss_3_normalization = self._build_tf_em_graph(A_log_ker, B_log_ker, pi_log_ker, A_log_ker_normal, B_log_ker_normal, pi_log_ker_normal)
<             
---
>                 self.gamma, self.bar_gamma, self.bar_gamma_pairwise, self.tilde_O_ph, self.loss_update, self.loss_scaled, self.loss_1, self.loss_1_normalization, self.loss_2, self.loss_2_normalization, self.loss_3, self.loss_3_normalization = self._build_tf_em_graph(
>                     A_log_ker, B_log_ker, pi_log_ker, A_log_ker_normal, B_log_ker_normal, pi_log_ker_normal)
> 
821,822c844,846
<                 self.loss_cooc, self.loss_cooc_update, self.A_stationary, self.omega = self._build_tf_coocs_graph(A_from_reps_hmmlearn, B_from_reps_hmmlearn, self.omega_gt_ph)
<             
---
>                 self.loss_cooc, self.loss_cooc_update, self.A_stationary, self.omega = self._build_tf_coocs_graph(
>                     A_from_reps_hmmlearn, B_from_reps_hmmlearn, self.omega_gt_ph)
> 
824c848
<     
---
> 
832c856
<                      
---
> 
834,835c858,859
<         X, n_seqs, max_seqlen = super(DenseHMM, self)._init(X, lengths=lengths)
<         
---
>         X, n_seqs, max_seqlen = super(GaussianDenseHMM, self)._init(X, lengths=lengths)
> 
839c863
<         
---
> 
842,843c866
<         
<     
---
> 
844a868
> 
846c870
<         
---
> 
849c873
<             
---
> 
851c875
<             train_input_dict = {self.gamma: stats['gamma'], 
---
>             train_input_dict = {self.gamma: stats['gamma'],
857c881
<             
---
> 
862,866c886,889
<         
<         #print(A, B, pi)
<         
<         
<     def _compute_metrics(self, X, lengths, stats, em_iter, ident, 
---
> 
>         # print(A, B, pi)
> 
>     def _compute_metrics(self, X, lengths, stats, em_iter, ident,
868,869c891
<         
<         
---
> 
871,872c893,894
<         log_dict = super(DenseHMM, self)._compute_metrics(X, lengths, stats, em_iter, ident, val, val_lengths)
<         
---
>         log_dict = super(GaussianDenseHMM, self)._compute_metrics(X, lengths, stats, em_iter, ident, val, val_lengths)
> 
874c896
<         
---
> 
878,879c900,902
<         log_dict['train_losses_standard'] = self._compute_loss_standard(X, lengths, bar_gamma, bar_gamma_pairwise, gamma)
<         
---
>         log_dict['train_losses_standard'] = self._compute_loss_standard(X, lengths, bar_gamma, bar_gamma_pairwise,
>                                                                         gamma)
> 
881c904
<             
---
> 
883,884c906,908
<             log_dict['test_losses_standard'] = self._compute_loss_standard(val, val_lengths, bar_gamma, bar_gamma_pairwise, gamma)
<             
---
>             log_dict['test_losses_standard'] = self._compute_loss_standard(val, val_lengths, bar_gamma,
>                                                                            bar_gamma_pairwise, gamma)
> 
889c913
<             
---
> 
891,893c915,919
<                 log_dict['test_gamma_losses'] = self._compute_loss(val, val_lengths, val_bar_gamma, val_bar_gamma_pairwise, val_gamma)
<                 log_dict['test_gamma_losses_standard'] = self._compute_loss_standard(val, val_lengths, val_bar_gamma, val_bar_gamma_pairwise, val_gamma) 
<     
---
>                 log_dict['test_gamma_losses'] = self._compute_loss(val, val_lengths, val_bar_gamma,
>                                                                    val_bar_gamma_pairwise, val_gamma)
>                 log_dict['test_gamma_losses_standard'] = self._compute_loss_standard(val, val_lengths, val_bar_gamma,
>                                                                                      val_bar_gamma_pairwise, val_gamma)
> 
895c921
<     
---
> 
896a923
> 
898c925
<         
---
> 
900c927
<         
---
> 
904,906c931,933
<         
<         tilde_B = log_B[:, X] # Has shape (n_hidden_states, seqs, max_seqlen)
<         
---
> 
>         tilde_B = log_B[:, X]  # Has shape (n_hidden_states, seqs, max_seqlen)
> 
909c936
<         loss3 = -np.einsum('jit,itj->', tilde_B, gamma) 
---
>         loss3 = -np.einsum('jit,itj->', tilde_B, gamma)
911c938
<         
---
> 
913,914c940
<             
<         
---
> 
916c942
<         
---
> 
919,920c945,946
<         
<         input_dict = {self.gamma: gamma, 
---
> 
>         input_dict = {self.gamma: gamma,
924,927c950,953
<                      }
<         losses = self.session.run([self.loss_1, self.loss_1_normalization, 
<                                self.loss_2, self.loss_2_normalization, 
<                                self.loss_3, self.loss_3_normalization], feed_dict=input_dict)
---
>                       }
>         losses = self.session.run([self.loss_1, self.loss_1_normalization,
>                                    self.loss_2, self.loss_2_normalization,
>                                    self.loss_3, self.loss_3_normalization], feed_dict=input_dict)
930,931c956
<         
<    
---
> 
935c960
<         
---
> 
936a962
> 
938c964
<         
---
> 
940c966
<         
---
> 
944c970
<         
---
> 
950c976
<         
---
> 
953c979
<                                              
---
> 
957c983
<             
---
> 
959c985
<                                              
---
> 
961c987
<          
---
> 
964c990
<         
---
> 
968c994
<         
---
> 
971c997
<             
---
> 
975,976c1001,1002
<             return A, B, compute_stationary(A, verbose=False) 
<           
---
>             return A, B, compute_stationary(A, verbose=False)
> 
979c1005
<         
---
> 
981c1007
<             
---
> 
984c1010
<             
---
> 
988c1014
<                      
---
> 
991c1017
<                      
---
> 
994c1020
<                      
---
> 
998c1024
<                      
---
> 
1003,1005c1029,1032
<                      
<         log_dict.update({'cooc_transmat': self.transmat_, 'cooc_emissionprob': self.emissionprob_, 'cooc_startprob': self.startprob_, 'cooc_omega': learned_omega})
<         
---
> 
>         log_dict.update({'cooc_transmat': self.transmat_, 'cooc_emissionprob': self.emissionprob_,
>                          'cooc_startprob': self.startprob_, 'cooc_omega': learned_omega})
> 
1008c1035
<                      
---
> 
1019c1046
<                 
---
> 
1021c1048
<     
---
> 
1024,1027d1050
< 
< 
< 
< 
